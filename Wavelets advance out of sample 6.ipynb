{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ed4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "from itertools import combinations\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================\n",
    "# SEMILLAS ALEATORIAS MEJORADAS\n",
    "# ==============================================================\n",
    "def set_random_seeds(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    if hasattr(th.backends, 'cudnn'):\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==============================================================\n",
    "# CONFIGURATION CON TIMESTEPS REALISTAS Y MÉTRICAS COMPLETAS + MÉTODOS CLÁSICOS\n",
    "# ==============================================================\n",
    "POINT_VALUE = 50\n",
    "COMMISSION_PER_TRADE = 0.25\n",
    "SPREAD = 0.25\n",
    "\n",
    "# 🚀 WAVELETS EXPANDIDAS (para responder a Reviewer #2)\n",
    "wavelet_families = {\n",
    "    'Daubechies': ['db1', 'db2', 'db4'],      \n",
    "    'Symlets': ['sym1', 'sym4'],        \n",
    "    'Coiflets': ['coif1', 'coif4'],\n",
    "    'Biorthogonal': ['bior1.1','bior2.2']\n",
    "}\n",
    "\n",
    "# 🚀 HYPERPARAMETERS REALISTAS - CORREGIDO CON MÁS VARIACIÓN\n",
    "HYPERPARAM_GRID = {\n",
    "    'PPO': {\n",
    "        'learning_rate': [3e-4, 1e-4],     \n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'n_steps': [2048],                 \n",
    "        'batch_size': [128]                \n",
    "    },\n",
    "    'A2C': {\n",
    "        'learning_rate': [1e-4, 5e-5, 3e-4],    # MÁS VARIACIÓN\n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'n_steps': [20, 50]                      # MÁS VARIACIÓN\n",
    "    },\n",
    "    'DQN': {\n",
    "        'learning_rate': [1e-4, 5e-5, 3e-4],    # MÁS VARIACIÓN\n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'buffer_size': [100000, 50000],          # MÁS VARIACIÓN\n",
    "        'exploration_fraction': [0.1, 0.2]      # MÁS VARIACIÓN\n",
    "    }\n",
    "}\n",
    "\n",
    "# 🚀 TIMESTEPS REALISTAS\n",
    "TRAINING_TIMESTEPS = {\n",
    "    'hyperparameter_tuning': 10000,    \n",
    "    'final_training': 50000,           \n",
    "    'validation_steps': 5000           \n",
    "}\n",
    "\n",
    "# 🚀 TODAS LAS COMBINACIONES DE FEATURES\n",
    "MAIN_FEATURE_COMBINATIONS = [\n",
    "    ('DIX',), ('GEX',), ('VIX',),\n",
    "    ('DIX', 'GEX'), ('DIX', 'VIX'), ('GEX', 'VIX'),\n",
    "    ('DIX', 'GEX', 'VIX')\n",
    "]\n",
    "\n",
    "# Paths\n",
    "file_path = r\"C:\\Users\\elect\\OneDrive\\Documentos\\Doctorado\\Articulo 4 peer review\\Data\\merged_data_with_vix_2.csv\"\n",
    "results_dir = r\"C:\\Users\\elect\\OneDrive\\Documentos\\Doctorado\\Articulo 4 peer review\\results_COMPLETE_WITH_CLASSICAL_METHODS\"\n",
    "\n",
    "# Create directories\n",
    "directories = ['graphs', 'csv', 'out_of_sample', 'academic_reports', 'denoising_analysis', 'wavelet_analysis']\n",
    "for dir_name in directories:\n",
    "    os.makedirs(os.path.join(results_dir, dir_name), exist_ok=True)\n",
    "\n",
    "print(\"🚀 COMPLETE CONFIGURATION WITH ALL IMPROVEMENTS FOR REVIEWERS:\")\n",
    "print(f\"📊 Wavelet families: {len(wavelet_families)} ({sum(len(w) for w in wavelet_families.values())} wavelets)\")\n",
    "print(f\"🤖 RL Algorithms: PPO, A2C, DQN\")  \n",
    "print(f\"🔧 Classical Methods: XGBoost, Random Forest, Logistic Regression, Buy&Hold, Moving Average\")\n",
    "print(f\"🔧 Feature combinations: {len(MAIN_FEATURE_COMBINATIONS)}\")\n",
    "print(f\"🧠 Feature Extractor: ADVANCED (Attention + BiLSTM + Positional Encoding)\")\n",
    "print(f\"📈 Strategy: LONG-ONLY (Buy/Hold - No Short Selling)\")\n",
    "print(f\"🎯 Training timesteps: {TRAINING_TIMESTEPS['final_training']:,}\")\n",
    "print(f\"📊 Metrics: Alpha, Beta, Sharpe, Sortino, Max DD, Win Rate, Max Loss, RoMaD, CAGR, Volatility, T-Stat, P-Value\")\n",
    "print(f\"🔬 Analysis: Denoising effectiveness, Wavelet level justification, Statistical significance\")\n",
    "\n",
    "# ==============================================================\n",
    "# DATA LOADING\n",
    "# ==============================================================\n",
    "print(\"\\n📊 Loading data...\")\n",
    "data = pd.read_csv(file_path)\n",
    "print(f\"📋 Dataset Description:\")\n",
    "print(f\"   • Shape: {data.shape}\")\n",
    "print(f\"   • Columns: {list(data.columns)}\")\n",
    "print(f\"   • Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "print(f\"   • Missing values: {data.isnull().sum().sum()}\")\n",
    "print(f\"   • Description: S&P 500 futures data with DIX, GEX, VIX indicators for algorithmic trading\")\n",
    "\n",
    "data.set_index('date', inplace=True)\n",
    "print(f\"✅ Data loaded and indexed by date\")\n",
    "\n",
    "def split_data_temporal(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_data = data.iloc[:train_end].copy()\n",
    "    val_data = data.iloc[train_end:val_end].copy()\n",
    "    test_data = data.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"📈 Temporal Split (No Data Leakage):\")\n",
    "    print(f\"   Train: {len(train_data)} samples ({train_data.index[0]} to {train_data.index[-1]})\")\n",
    "    print(f\"   Val: {len(val_data)} samples ({val_data.index[0]} to {val_data.index[-1]})\")\n",
    "    print(f\"   Test: {len(test_data)} samples ({test_data.index[0]} to {test_data.index[-1]})\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ==============================================================\n",
    "# TRADING ENVIRONMENT - LONG ONLY - CORREGIDO CON RESET COMPLETO\n",
    "# ==============================================================\n",
    "class SimpleTradingEnv(gym.Env):\n",
    "    def __init__(self, indicator_data, price_data):\n",
    "        super(SimpleTradingEnv, self).__init__()\n",
    "        \n",
    "        self.indicator_data = np.array(indicator_data, dtype=np.float32)\n",
    "        self.price_data = np.array(price_data, dtype=np.float32)\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_len = min(len(self.indicator_data), len(self.price_data))\n",
    "        self.indicator_data = self.indicator_data[:min_len]\n",
    "        self.price_data = self.price_data[:min_len]\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.initial_balance = 100000\n",
    "        self.reset()\n",
    "        \n",
    "        # Observation space\n",
    "        if len(self.indicator_data.shape) == 1:\n",
    "            obs_shape = (1,)\n",
    "        else:\n",
    "            obs_shape = (self.indicator_data.shape[1],)\n",
    "            \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-10.0, high=10.0, shape=obs_shape, dtype=np.float32\n",
    "        )\n",
    "        # ONLY 2 ACTIONS: 0: Hold/Sell, 1: Buy\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "    def reset(self):\n",
    "        # RESET COMPLETO de todas las variables\n",
    "        self.current_step = 0\n",
    "        self.balance = float(self.initial_balance)  # Ensure float\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0  # Ensure float\n",
    "        self.balance_history = [float(self.initial_balance)]\n",
    "        self.trades = 0\n",
    "        \n",
    "        # IMPORTANTE: Resetear también el estado interno\n",
    "        self._last_action = None\n",
    "        self._episode_steps = 0\n",
    "        \n",
    "        if len(self.indicator_data.shape) == 1:\n",
    "            return np.array([self.indicator_data[0]], dtype=np.float32)\n",
    "        else:\n",
    "            return self.indicator_data[0].astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.price_data) - 1:\n",
    "            # Close any open position at the end\n",
    "            if self.position == 1:\n",
    "                current_price = self.price_data[self.current_step]\n",
    "                # Sell position (incluye spread y comisiones)\n",
    "                pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "                self.balance += pnl\n",
    "                self.position = 0\n",
    "                self.trades += 1\n",
    "            \n",
    "            self.balance_history.append(self.balance)\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "            return obs, 0, True, {\"balance\": self.balance, \"trades\": self.trades}\n",
    "        \n",
    "        current_price = self.price_data[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # LONG ONLY ACTION LOGIC\n",
    "        if action == 1:  # BUY\n",
    "            if self.position == 0:  # No position -> Open long\n",
    "                self.position = 1\n",
    "                self.entry_price = current_price + SPREAD  # Include spread\n",
    "                self.balance -= COMMISSION_PER_TRADE\n",
    "                reward -= COMMISSION_PER_TRADE / 1000\n",
    "                self.trades += 1\n",
    "                \n",
    "        elif action == 0:  # HOLD/SELL\n",
    "            if self.position == 1:  # Long position -> Close long\n",
    "                pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "                self.balance += pnl\n",
    "                reward += pnl / 1000\n",
    "                self.position = 0\n",
    "                self.trades += 1\n",
    "        \n",
    "        # Calculate current balance (with unrealized PnL if in position)\n",
    "        current_balance = self.balance\n",
    "        if self.position == 1:\n",
    "            unrealized_pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE\n",
    "            current_balance += unrealized_pnl\n",
    "        \n",
    "        self.balance_history.append(current_balance)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.price_data) - 1\n",
    "        \n",
    "        # Next observation\n",
    "        if not done:\n",
    "            if len(self.indicator_data.shape) == 1:\n",
    "                obs = np.array([self.indicator_data[self.current_step]], dtype=np.float32)\n",
    "            else:\n",
    "                obs = self.indicator_data[self.current_step].astype(np.float32)\n",
    "        else:\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        info = {\"balance\": current_balance, \"trades\": self.trades, \"position\": self.position}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def get_balance_history(self):\n",
    "        return self.balance_history.copy()\n",
    "\n",
    "# ==============================================================\n",
    "# 🧠 ADVANCED FEATURE EXTRACTOR\n",
    "# ==============================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = th.arange(max_len).unsqueeze(1)\n",
    "        div_term = th.exp(th.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = th.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = th.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = th.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = th.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AdvancedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, observation_space, features_dim=64, n_heads=4, dropout=0.1):\n",
    "        super(AdvancedFeatureExtractor, self).__init__()\n",
    "        self.features_dim = features_dim\n",
    "        input_dim = observation_space.shape[0]\n",
    "        \n",
    "        print(f\"🧠 Creating AdvancedFeatureExtractor with input_dim={input_dim}, features_dim={features_dim}\")\n",
    "        \n",
    "        hidden_dim = min(128, max(64, input_dim * 16))\n",
    "        \n",
    "        self.ff_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        attention_dim = hidden_dim // 2\n",
    "        self.attention = MultiheadAttention(\n",
    "            embed_dim=attention_dim, \n",
    "            num_heads=min(n_heads, attention_dim // 16),\n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attention_norm = nn.LayerNorm(attention_dim)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            attention_dim, \n",
    "            attention_dim // 2, \n",
    "            bidirectional=True, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if dropout > 0 else 0\n",
    "        )\n",
    "        self.lstm_norm = nn.LayerNorm(attention_dim)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(attention_dim, features_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(features_dim * 2, features_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(attention_dim, dropout=dropout)\n",
    "        \n",
    "        print(f\"✅ AdvancedFeatureExtractor created successfully!\")\n",
    "        \n",
    "    def forward(self, observations):\n",
    "        original_shape = observations.shape\n",
    "        if len(original_shape) == 1:\n",
    "            observations = observations.unsqueeze(0)\n",
    "        \n",
    "        x = self.ff_encoder(observations)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        x_attn, _ = self.attention(x, x, x)\n",
    "        x = self.attention_norm(x + x_attn)\n",
    "        \n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        x = self.lstm_norm(lstm_out)\n",
    "        \n",
    "        x = x.squeeze(1)\n",
    "        features = self.projection(x)\n",
    "        \n",
    "        if len(original_shape) == 1:\n",
    "            features = features.squeeze(0)\n",
    "            \n",
    "        return features\n",
    "\n",
    "# ==============================================================\n",
    "# 📊 COMPLETE METRICS CALCULATION\n",
    "# ==============================================================\n",
    "def calculate_complete_metrics(balance_history, price_data, benchmark_returns=None):\n",
    "    \"\"\"Calculate ALL requested metrics including Alpha, Beta, Sortino, etc.\"\"\"\n",
    "    \n",
    "    if len(balance_history) < 2:\n",
    "        return {\n",
    "            \"Alpha\": 0.0, \"Beta\": 0.0, \"Sharpe_Ratio\": 0.0, \"Sortino_Ratio\": 0.0,\n",
    "            \"Max_Drawdown\": 0.0, \"Win_Rate\": 0.0, \"Max_Loss\": 0.0, \"RoMaD\": 0.0,\n",
    "            \"CAGR\": 0.0, \"Volatility\": 0.0, \"T_Statistic\": 0.0, \"P_Value\": 1.0,\n",
    "            \"Total_Return\": 0.0\n",
    "        }\n",
    "    \n",
    "    balance_history = np.array(balance_history)\n",
    "    returns = np.diff(balance_history) / balance_history[:-1]\n",
    "    returns = returns[np.isfinite(returns)]\n",
    "    \n",
    "    if len(returns) < 2:\n",
    "        return {\n",
    "            \"Alpha\": 0.0, \"Beta\": 0.0, \"Sharpe_Ratio\": 0.0, \"Sortino_Ratio\": 0.0,\n",
    "            \"Max_Drawdown\": 0.0, \"Win_Rate\": 0.0, \"Max_Loss\": 0.0, \"RoMaD\": 0.0,\n",
    "            \"CAGR\": 0.0, \"Volatility\": 0.0, \"T_Statistic\": 0.0, \"P_Value\": 1.0,\n",
    "            \"Total_Return\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_return = (balance_history[-1] - balance_history[0]) / balance_history[0]\n",
    "    \n",
    "    # CAGR\n",
    "    n_years = len(balance_history) / 252\n",
    "    if n_years > 0 and balance_history[0] > 0:\n",
    "        cagr = (balance_history[-1] / balance_history[0]) ** (1 / n_years) - 1\n",
    "    else:\n",
    "        cagr = 0\n",
    "    \n",
    "    # Volatility (Annualized)\n",
    "    volatility = np.std(returns) * np.sqrt(252) if len(returns) > 1 else 0\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    risk_free_rate = 0.02\n",
    "    excess_returns = returns - (risk_free_rate / 252)\n",
    "    sharpe_ratio = np.mean(excess_returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
    "    \n",
    "    # Drawdown metrics\n",
    "    peak = np.maximum.accumulate(balance_history)\n",
    "    drawdown = (peak - balance_history) / np.maximum(peak, 1)\n",
    "    max_drawdown = np.max(drawdown)\n",
    "    \n",
    "    # Sortino Ratio\n",
    "    negative_returns = returns[returns < 0]\n",
    "    downside_std = np.std(negative_returns) * np.sqrt(252) if len(negative_returns) > 1 else 0.001\n",
    "    sortino_ratio = np.mean(excess_returns) * np.sqrt(252) / downside_std if downside_std > 0 else 0\n",
    "    \n",
    "    # Win Rate\n",
    "    positive_returns = returns[returns > 0]\n",
    "    win_rate = len(positive_returns) / len(returns) if len(returns) > 0 else 0\n",
    "    \n",
    "    # Max Loss\n",
    "    max_loss = np.min(returns) if len(returns) > 0 else 0\n",
    "    \n",
    "    # RoMaD\n",
    "    romad = cagr / max_drawdown if max_drawdown > 0 else 0\n",
    "    \n",
    "    # T-statistic and P-value\n",
    "    if len(returns) > 1:\n",
    "        t_stat, p_value = stats.ttest_1samp(returns, 0)\n",
    "        t_stat = float(t_stat) if not np.isnan(t_stat) else 0\n",
    "        p_value = float(p_value) if not np.isnan(p_value) else 1\n",
    "    else:\n",
    "        t_stat, p_value = 0, 1\n",
    "    \n",
    "    # Alpha and Beta\n",
    "    if benchmark_returns is not None and len(benchmark_returns) == len(returns):\n",
    "        try:\n",
    "            covariance = np.cov(returns, benchmark_returns)[0][1]\n",
    "            benchmark_variance = np.var(benchmark_returns)\n",
    "            beta = covariance / benchmark_variance if benchmark_variance > 0 else 0\n",
    "            \n",
    "            benchmark_mean = np.mean(benchmark_returns)\n",
    "            returns_mean = np.mean(returns)\n",
    "            alpha = returns_mean - (risk_free_rate/252 + beta * (benchmark_mean - risk_free_rate/252))\n",
    "            alpha = alpha * 252  # Annualize\n",
    "            \n",
    "        except:\n",
    "            alpha, beta = 0, 0\n",
    "    else:\n",
    "        market_return = 0.08 / 252\n",
    "        market_returns = np.full(len(returns), market_return)\n",
    "        \n",
    "        try:\n",
    "            covariance = np.cov(returns, market_returns)[0][1]\n",
    "            market_variance = np.var(market_returns)\n",
    "            beta = covariance / market_variance if market_variance > 0 else 0\n",
    "            \n",
    "            returns_mean = np.mean(returns)\n",
    "            alpha = returns_mean - (risk_free_rate/252 + beta * (market_return - risk_free_rate/252))\n",
    "            alpha = alpha * 252\n",
    "        except:\n",
    "            alpha, beta = 0, 0\n",
    "    \n",
    "    return {\n",
    "        \"Alpha\": float(alpha),\n",
    "        \"Beta\": float(beta),\n",
    "        \"Sharpe_Ratio\": float(sharpe_ratio),\n",
    "        \"Sortino_Ratio\": float(sortino_ratio),\n",
    "        \"Max_Drawdown\": float(max_drawdown),\n",
    "        \"Win_Rate\": float(win_rate),\n",
    "        \"Max_Loss\": float(max_loss),\n",
    "        \"RoMaD\": float(romad),\n",
    "        \"CAGR\": float(cagr),\n",
    "        \"Volatility\": float(volatility),\n",
    "        \"T_Statistic\": float(t_stat),\n",
    "        \"P_Value\": float(p_value),\n",
    "        \"Total_Return\": float(total_return)\n",
    "    }\n",
    "\n",
    "# ==============================================================\n",
    "# 🔬 DENOISING ANALYSIS\n",
    "# ==============================================================\n",
    "def analyze_denoising_effectiveness(original_data, denoised_data, wavelet_name, save_path):\n",
    "    \"\"\"Analyze and visualize denoising effectiveness\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original vs Denoised\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(original_data[:200], label='Original', alpha=0.7, linewidth=1)\n",
    "    plt.plot(denoised_data[:200], label='Denoised', alpha=0.8, linewidth=1.5)\n",
    "    plt.title(f'Signal Comparison - {wavelet_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Noise removed\n",
    "    plt.subplot(2, 3, 2)\n",
    "    noise = original_data - denoised_data\n",
    "    plt.plot(noise[:200], color='red', alpha=0.7)\n",
    "    plt.title('Removed Noise')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # FFT Analysis\n",
    "    plt.subplot(2, 3, 3)\n",
    "    freqs_orig = np.fft.fftfreq(len(original_data))\n",
    "    fft_orig = np.abs(np.fft.fft(original_data))\n",
    "    fft_clean = np.abs(np.fft.fft(denoised_data))\n",
    "    plt.semilogy(freqs_orig[:len(freqs_orig)//2], fft_orig[:len(fft_orig)//2], alpha=0.7, label='Original')\n",
    "    plt.semilogy(freqs_orig[:len(freqs_orig)//2], fft_clean[:len(fft_clean)//2], alpha=0.8, label='Denoised')\n",
    "    plt.title('Frequency Domain Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wavelet decomposition visualization\n",
    "    plt.subplot(2, 3, 4)\n",
    "    coeffs = pywt.wavedec(original_data, wavelet_name, level=2)\n",
    "    plt.plot(coeffs[0][:100], label='Approximation', linewidth=1.5)\n",
    "    plt.title('Approximation Coefficients (L2)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(coeffs[1][:50], label='Detail L1', alpha=0.8)\n",
    "    plt.plot(coeffs[2][:50], label='Detail L2', alpha=0.8)\n",
    "    plt.title('Detail Coefficients')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics calculation\n",
    "    signal_power = np.var(denoised_data)\n",
    "    noise_power = np.var(noise)\n",
    "    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')\n",
    "    noise_reduction = (np.std(noise)/np.std(original_data)*100)\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.text(0.1, 0.8, f'SNR Improvement: {snr:.2f} dB', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.7, f'Noise Reduction: {noise_reduction:.1f}%', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.6, f'Wavelet: {wavelet_name}', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.5, f'Decomposition Level: 2', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.4, f'Threshold: Conservative (0.1 * σ)', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.3, f'Method: Soft thresholding', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.2, f'Justification: Level 2 provides good', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.1, f'balance between denoising and signal preservation', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.axis('off')\n",
    "    plt.title('Denoising Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return snr, noise_power, noise_reduction\n",
    "\n",
    "# ==============================================================\n",
    "# 📊 STATISTICAL SIGNIFICANCE ANALYSIS\n",
    "# ==============================================================\n",
    "def statistical_significance_analysis(results_df, save_path):\n",
    "    \"\"\"Perform statistical significance tests\"\"\"\n",
    "    \n",
    "    algorithms = results_df['Algorithm'].unique()\n",
    "    metrics = ['Sharpe_Ratio', 'Total_Return', 'Max_Drawdown', 'Alpha', 'Sortino_Ratio']\n",
    "    \n",
    "    significance_results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        significance_results[metric] = {}\n",
    "        for i, algo1 in enumerate(algorithms):\n",
    "            for algo2 in algorithms[i+1:]:\n",
    "                data1 = results_df[results_df['Algorithm'] == algo1][metric]\n",
    "                data2 = results_df[results_df['Algorithm'] == algo2][metric]\n",
    "                \n",
    "                if len(data1) > 1 and len(data2) > 1:\n",
    "                    t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "                    significance_results[metric][f'{algo1}_vs_{algo2}'] = {\n",
    "                        't_statistic': t_stat,\n",
    "                        'p_value': p_value,\n",
    "                        'significant': p_value < 0.05,\n",
    "                        'mean_diff': float(data1.mean() - data2.mean())\n",
    "                    }\n",
    "    \n",
    "    # Create significance table and save\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"STATISTICAL SIGNIFICANCE ANALYSIS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            f.write(f\"\\n{metric.upper()}:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for comparison, stats_result in significance_results[metric].items():\n",
    "                significance = \"SIGNIFICANT\" if stats_result['significant'] else \"NOT SIGNIFICANT\"\n",
    "                f.write(f\"{comparison}: p={stats_result['p_value']:.4f} ({significance})\\n\")\n",
    "                f.write(f\"  Mean difference: {stats_result['mean_diff']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"✅ Statistical significance analysis saved to: {save_path}\")\n",
    "    return significance_results\n",
    "\n",
    "# ==============================================================\n",
    "# 🤖 CLASSICAL METHODS IMPLEMENTATION\n",
    "# ==============================================================\n",
    "def implement_classical_methods(train_features, train_labels, test_features, test_prices):\n",
    "    \"\"\"Implement classical ML methods including XGBoost\"\"\"\n",
    "    \n",
    "    classical_results = {}\n",
    "    \n",
    "    # 1. XGBoost\n",
    "    print(\"  🚀 Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "    xgb_predictions = xgb_model.predict(test_features)\n",
    "    xgb_balance = simulate_trading_strategy(xgb_predictions, test_prices)\n",
    "    classical_results['XGBoost'] = calculate_complete_metrics(xgb_balance, test_prices)\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"  🌲 Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(train_features, train_labels)\n",
    "    rf_predictions = rf_model.predict(test_features)\n",
    "    rf_balance = simulate_trading_strategy(rf_predictions, test_prices)\n",
    "    classical_results['Random_Forest'] = calculate_complete_metrics(rf_balance, test_prices)\n",
    "    \n",
    "    # 3. Logistic Regression\n",
    "    print(\"  📊 Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(random_state=42)\n",
    "    lr_model.fit(train_features, train_labels)\n",
    "    lr_predictions = lr_model.predict(test_features)\n",
    "    lr_balance = simulate_trading_strategy(lr_predictions, test_prices)\n",
    "    classical_results['Logistic_Regression'] = calculate_complete_metrics(lr_balance, test_prices)\n",
    "    \n",
    "    # 4. Buy and Hold\n",
    "    print(\"  💰 Calculating Buy & Hold...\")\n",
    "    buy_hold_return = (test_prices[-1] - test_prices[0]) / test_prices[0]\n",
    "    buy_hold_balance = [100000 * (1 + (price - test_prices[0]) / test_prices[0]) for price in test_prices]\n",
    "    classical_results['Buy_and_Hold'] = calculate_complete_metrics(buy_hold_balance, test_prices)\n",
    "    \n",
    "    # 5. Moving Average Crossover\n",
    "    print(\"  📈 Calculating Moving Average Strategy...\")\n",
    "    ma_short = pd.Series(test_prices).rolling(20).mean()\n",
    "    ma_long = pd.Series(test_prices).rolling(50).mean()\n",
    "    ma_signals = np.where(ma_short > ma_long, 1, 0)\n",
    "    ma_balance = simulate_trading_strategy(ma_signals, test_prices)\n",
    "    classical_results['Moving_Average'] = calculate_complete_metrics(ma_balance, test_prices)\n",
    "    \n",
    "    return classical_results\n",
    "\n",
    "def create_trading_labels(price_data, lookforward=5):\n",
    "    \"\"\"Create trading labels for supervised learning\"\"\"\n",
    "    labels = []\n",
    "    for i in range(len(price_data) - lookforward):\n",
    "        future_return = (price_data[i + lookforward] - price_data[i]) / price_data[i]\n",
    "        labels.append(1 if future_return > 0.002 else 0)  # 0.2% threshold\n",
    "    \n",
    "    return np.array(labels)\n",
    "\n",
    "def simulate_trading_strategy(signals, prices):\n",
    "    \"\"\"Simulate trading strategy given signals and prices\"\"\"\n",
    "    balance = 100000\n",
    "    position = 0\n",
    "    balance_history = [balance]\n",
    "    \n",
    "    for i, signal in enumerate(signals):\n",
    "        current_price = prices[i]\n",
    "        \n",
    "        if signal == 1 and position == 0:  # Buy signal\n",
    "            position = 1\n",
    "            entry_price = current_price + SPREAD\n",
    "            balance -= COMMISSION_PER_TRADE\n",
    "        elif signal == 0 and position == 1:  # Sell signal\n",
    "            pnl = (current_price - entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "            balance += pnl\n",
    "            position = 0\n",
    "        \n",
    "        # Calculate current balance\n",
    "        current_balance = balance\n",
    "        if position == 1:\n",
    "            unrealized_pnl = (current_price - entry_price - SPREAD) * POINT_VALUE\n",
    "            current_balance += unrealized_pnl\n",
    "        \n",
    "        balance_history.append(current_balance)\n",
    "    \n",
    "    return balance_history\n",
    "\n",
    "# ==============================================================\n",
    "# WAVELET PROCESSING\n",
    "# ==============================================================\n",
    "def simple_wavelet_denoise(series, wavelet):\n",
    "    \"\"\"Simplified wavelet denoising with analysis\"\"\"\n",
    "    try:\n",
    "        coeffs = pywt.wavedec(series, wavelet, level=2)\n",
    "        threshold = np.std(coeffs[-1]) * 0.1\n",
    "        coeffs[1:] = [pywt.threshold(c, threshold, 'soft') for c in coeffs[1:]]\n",
    "        return pywt.waverec(coeffs, wavelet)[:len(series)]\n",
    "    except:\n",
    "        return series\n",
    "\n",
    "# ==============================================================\n",
    "# HYPERPARAMETER TUNING - CORREGIDO CON MÁS COMBINACIONES\n",
    "# ==============================================================\n",
    "def realistic_hyperparameter_tuning(algo_class, train_features, val_features, train_prices, val_prices):\n",
    "    \"\"\"Hyperparameter tuning with realistic timesteps\"\"\"\n",
    "    \n",
    "    param_grid = HYPERPARAM_GRID[algo_class.__name__]\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    # CORREGIDO: Generar más combinaciones para A2C y DQN\n",
    "    if algo_class.__name__ == 'PPO':\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][0], 'batch_size': param_grid['batch_size'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'n_steps': param_grid['n_steps'][0], 'batch_size': param_grid['batch_size'][0]}\n",
    "        ]\n",
    "    elif algo_class.__name__ == 'A2C':\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'n_steps': param_grid['n_steps'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][2], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][1]},  # Nueva combinación\n",
    "        ]\n",
    "    else:  # DQN\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'buffer_size': param_grid['buffer_size'][0], 'exploration_fraction': param_grid['exploration_fraction'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'buffer_size': param_grid['buffer_size'][0], 'exploration_fraction': param_grid['exploration_fraction'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][2], 'gamma': param_grid['gamma'][0], \n",
    "             'buffer_size': param_grid['buffer_size'][1], 'exploration_fraction': param_grid['exploration_fraction'][1]},  # Nueva combinación\n",
    "        ]\n",
    "    \n",
    "    print(f\"🔍 Testing {len(combinations)} combinations with {TRAINING_TIMESTEPS['hyperparameter_tuning']:,} timesteps\")\n",
    "    \n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"      ⚙️ {i+1}/{len(combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            train_env = DummyVecEnv([lambda: SimpleTradingEnv(train_features.copy(), train_prices.copy())])\n",
    "            train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "            \n",
    "            policy_kwargs = dict(\n",
    "                features_extractor_class=AdvancedFeatureExtractor,\n",
    "                features_extractor_kwargs=dict(features_dim=64, n_heads=4, dropout=0.1)\n",
    "            )\n",
    "            \n",
    "            model = algo_class('MlpPolicy', train_env, verbose=0, policy_kwargs=policy_kwargs, **params)\n",
    "            model.learn(total_timesteps=TRAINING_TIMESTEPS['hyperparameter_tuning'])\n",
    "            \n",
    "            # Validation\n",
    "            val_env = DummyVecEnv([lambda: SimpleTradingEnv(val_features.copy(), val_prices.copy())])\n",
    "            val_env = VecNormalize(val_env, norm_obs=True, norm_reward=False, training=False)\n",
    "            # CORREGIDO: Copiar estadísticas de normalización (no solo referencia)\n",
    "            val_env.obs_rms = train_env.obs_rms.copy()\n",
    "            \n",
    "            obs = val_env.reset()\n",
    "            val_balance = [100000]\n",
    "            done = [False]\n",
    "            \n",
    "            while not done[0]:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, _, done, info = val_env.step(action)\n",
    "                val_balance.append(info[0]['balance'])\n",
    "            \n",
    "            val_metrics = calculate_complete_metrics(val_balance, val_prices)\n",
    "            score = val_metrics['Sharpe_Ratio'] + val_metrics['Sortino_Ratio'] + val_metrics['Total_Return'] - val_metrics['Max_Drawdown']\n",
    "            \n",
    "            print(f\"        📊 Score: {score:.3f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "                best_params = params.copy()\n",
    "                best_params['train_env'] = train_env\n",
    "                best_params['score'] = score\n",
    "            \n",
    "            val_env.close()\n",
    "            if best_params.get('train_env') != train_env:\n",
    "                train_env.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"        ❌ Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# ==============================================================\n",
    "# MAIN PROCESSING LOOP - CORREGIDO CON SEMILLAS ÚNICAS\n",
    "# ==============================================================\n",
    "print(\"\\n🚀 Starting COMPLETE ANALYSIS with ALL IMPROVEMENTS...\")\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = split_data_temporal(data)\n",
    "benchmark_returns = np.diff(test_data['PRICE'].values) / test_data['PRICE'].values[:-1]\n",
    "\n",
    "# Initialize results\n",
    "all_rl_results = []\n",
    "all_classical_results = []\n",
    "denoising_analysis_results = []\n",
    "\n",
    "total_experiments = sum(len(wavelets) for wavelets in wavelet_families.values()) * 3 * len(MAIN_FEATURE_COMBINATIONS)\n",
    "current_experiment = 0\n",
    "\n",
    "print(f\"\\n📊 TOTAL EXPERIMENTS TO RUN: {total_experiments}\")\n",
    "print(f\"⏱️ Estimated time: ~{total_experiments * 8 / 60:.1f} hours\")\n",
    "\n",
    "# Process each family\n",
    "for family_name, wavelets in wavelet_families.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"🌊 PROCESSING: {family_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for wavelet in wavelets:\n",
    "        print(f\"\\n📡 Wavelet: {wavelet}\")\n",
    "        \n",
    "        try:\n",
    "            # Process all datasets\n",
    "            processed_data = {}\n",
    "            \n",
    "            for dataset_name, dataset in [('train', train_data), ('val', val_data), ('test', test_data)]:\n",
    "                price_data = dataset['PRICE'].values\n",
    "                indicator_data = dataset[['DIX', 'GEX', 'VIX']].values\n",
    "                \n",
    "                # Denoising with analysis\n",
    "                denoised_data = []\n",
    "                for col_idx, col_name in enumerate(['DIX', 'GEX', 'VIX']):\n",
    "                    series = indicator_data[:, col_idx]\n",
    "                    if np.any(np.isnan(series)):\n",
    "                        series = pd.Series(series).fillna(method='ffill').fillna(0).values\n",
    "                    \n",
    "                    denoised = simple_wavelet_denoise(series, wavelet)\n",
    "                    denoised_data.append(denoised)\n",
    "                    \n",
    "                    # Analyze denoising effectiveness for test data only\n",
    "                    if dataset_name == 'test':\n",
    "                        denoising_save_path = os.path.join(results_dir, 'denoising_analysis', \n",
    "                                                         f'denoising_{wavelet}_{col_name}.png')\n",
    "                        snr, noise_power, noise_reduction = analyze_denoising_effectiveness(\n",
    "                            series, denoised, wavelet, denoising_save_path)\n",
    "                        \n",
    "                        denoising_analysis_results.append({\n",
    "                            'Wavelet': wavelet,\n",
    "                            'Indicator': col_name,\n",
    "                            'SNR_Improvement': snr,\n",
    "                            'Noise_Reduction_Percent': noise_reduction,\n",
    "                            'Noise_Power': noise_power\n",
    "                        })\n",
    "                \n",
    "                denoised_data = np.column_stack(denoised_data)\n",
    "                \n",
    "                # Normalization\n",
    "                if dataset_name == 'train':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    normalized_data = scaler.fit_transform(denoised_data)\n",
    "                    processed_data['scaler'] = scaler\n",
    "                else:\n",
    "                    normalized_data = processed_data['scaler'].transform(denoised_data)\n",
    "                \n",
    "                processed_data[dataset_name] = {\n",
    "                    'price': price_data,\n",
    "                    'indicators': normalized_data\n",
    "                }\n",
    "            \n",
    "            # Feature mapping\n",
    "            feature_mapping = {\n",
    "                'DIX': [0], 'GEX': [1], 'VIX': [2],\n",
    "                'DIX_GEX': [0, 1], 'DIX_VIX': [0, 2], 'GEX_VIX': [1, 2],\n",
    "                'DIX_GEX_VIX': [0, 1, 2]\n",
    "            }\n",
    "            \n",
    "            # Process RL algorithms\n",
    "            algorithms = {'PPO': PPO, 'A2C': A2C, 'DQN': DQN}\n",
    "            \n",
    "            for algo_name, algo_class in algorithms.items():\n",
    "                print(f\"\\n  🤖 {algo_name}\")\n",
    "                \n",
    "                for feature_comb in MAIN_FEATURE_COMBINATIONS:\n",
    "                    current_experiment += 1\n",
    "                    \n",
    "                    if len(feature_comb) == 1:\n",
    "                        feature_name = feature_comb[0]\n",
    "                    else:\n",
    "                        feature_name = '_'.join(feature_comb)\n",
    "                    \n",
    "                    feature_indices = feature_mapping[feature_name]\n",
    "                    \n",
    "                    # CORREGIDO: Semilla única por algoritmo, wavelet y features\n",
    "                    algo_seed = hash(f\"{algo_name}_{wavelet}_{feature_name}_{current_experiment}\") % (2**32)\n",
    "                    set_random_seeds(algo_seed)\n",
    "                    \n",
    "                    print(f\"    📊 [{current_experiment}/{total_experiments}] {feature_name} (seed: {algo_seed})\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract features\n",
    "                        train_features = processed_data['train']['indicators'][:, feature_indices]\n",
    "                        val_features = processed_data['val']['indicators'][:, feature_indices]\n",
    "                        test_features = processed_data['test']['indicators'][:, feature_indices]\n",
    "                        \n",
    "                        # Hyperparameter tuning\n",
    "                        best_model, best_params = realistic_hyperparameter_tuning(\n",
    "                            algo_class, train_features, val_features,\n",
    "                            processed_data['train']['price'], processed_data['val']['price']\n",
    "                        )\n",
    "                        \n",
    "                        if best_model is None:\n",
    "                            print(f\"      ❌ No valid model\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Final training\n",
    "                        print(f\"      🎯 Final training with {TRAINING_TIMESTEPS['final_training']:,} timesteps...\")\n",
    "                        train_env = DummyVecEnv([lambda: SimpleTradingEnv(train_features.copy(), processed_data['train']['price'].copy())])\n",
    "                        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "                        \n",
    "                        policy_kwargs = dict(\n",
    "                            features_extractor_class=AdvancedFeatureExtractor,\n",
    "                            features_extractor_kwargs=dict(features_dim=64, n_heads=4, dropout=0.1)\n",
    "                        )\n",
    "                        \n",
    "                        final_params = {k: v for k, v in best_params.items() if k not in ['train_env', 'score']}\n",
    "                        model = algo_class('MlpPolicy', train_env, verbose=0, policy_kwargs=policy_kwargs, **final_params)\n",
    "                        model.learn(total_timesteps=TRAINING_TIMESTEPS['final_training'])\n",
    "                        \n",
    "                        # Test evaluation\n",
    "                        print(f\"      📊 Testing...\")\n",
    "                        test_env = SimpleTradingEnv(test_features.copy(), processed_data['test']['price'].copy())\n",
    "                        \n",
    "                        obs = test_env.reset()\n",
    "                        done = False\n",
    "                        step_count = 0\n",
    "                        \n",
    "                        while not done and step_count < len(test_features):\n",
    "                            if hasattr(train_env, 'obs_rms'):\n",
    "                                normalized_obs = (obs - train_env.obs_rms.mean) / np.sqrt(train_env.obs_rms.var + 1e-8)\n",
    "                            else:\n",
    "                                normalized_obs = obs\n",
    "                            \n",
    "                            action, _ = model.predict(normalized_obs.reshape(1, -1), deterministic=True)\n",
    "                            obs, reward, done, info = test_env.step(action[0] if hasattr(action, '__getitem__') else action)\n",
    "                            step_count += 1\n",
    "                        \n",
    "                        # Results\n",
    "                        test_balance = test_env.get_balance_history()\n",
    "                        test_metrics = calculate_complete_metrics(test_balance, processed_data['test']['price'], benchmark_returns)\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'Method_Type': 'Reinforcement_Learning',\n",
    "                            'Wavelet_Family': family_name,\n",
    "                            'Wavelet': wavelet,\n",
    "                            'Algorithm': algo_name,\n",
    "                            'Features': feature_name,\n",
    "                            'Feature_Count': len(feature_indices),\n",
    "                            'Feature_Extractor': 'Advanced',\n",
    "                            'Strategy': 'Long-Only',\n",
    "                            'Training_Timesteps': TRAINING_TIMESTEPS['final_training'],\n",
    "                            'Random_Seed': algo_seed,  # AÑADIDO\n",
    "                            **test_metrics,\n",
    "                            'Best_Learning_Rate': best_params.get('learning_rate', 0),\n",
    "                            'Best_Gamma': best_params.get('gamma', 0),\n",
    "                            'Validation_Score': best_params.get('score', 0)\n",
    "                        }\n",
    "                        \n",
    "                        all_rl_results.append(result)\n",
    "                        \n",
    "                        print(f\"      ✅ Sharpe: {test_metrics['Sharpe_Ratio']:.3f}, Return: {test_metrics['Total_Return']*100:.1f}%\")\n",
    "                        \n",
    "                        # Cleanup\n",
    "                        train_env.close()\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      ❌ Error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            # Classical methods (once per wavelet, using best feature combination)\n",
    "            print(f\"\\n  🏛️ Classical Methods for {wavelet}\")\n",
    "            try:\n",
    "                best_features = processed_data['train']['indicators']  # Use all features\n",
    "                test_features_classical = processed_data['test']['indicators']\n",
    "                \n",
    "                # Create labels for supervised learning\n",
    "                train_labels = create_trading_labels(processed_data['train']['price'])\n",
    "                # Ensure features and labels have same length\n",
    "                min_len = min(len(best_features), len(train_labels))\n",
    "                best_features = best_features[:min_len]\n",
    "                train_labels = train_labels[:min_len]\n",
    "                \n",
    "                classical_results = implement_classical_methods(\n",
    "                    best_features, train_labels, test_features_classical, processed_data['test']['price']\n",
    "                )\n",
    "                \n",
    "                for method_name, metrics in classical_results.items():\n",
    "                    classical_result = {\n",
    "                        'Method_Type': 'Classical',\n",
    "                        'Wavelet_Family': family_name,\n",
    "                        'Wavelet': wavelet,\n",
    "                        'Algorithm': method_name,\n",
    "                        'Features': 'DIX_GEX_VIX',\n",
    "                        'Feature_Count': 3,\n",
    "                        'Feature_Extractor': 'None',\n",
    "                        'Strategy': 'Long-Only',\n",
    "                        'Training_Timesteps': 0,\n",
    "                        'Random_Seed': 0,  # AÑADIDO\n",
    "                        **metrics,\n",
    "                        'Best_Learning_Rate': 0,\n",
    "                        'Best_Gamma': 0,\n",
    "                        'Validation_Score': 0\n",
    "                    }\n",
    "                    all_classical_results.append(classical_result)\n",
    "                \n",
    "                print(f\"      ✅ Classical methods completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ❌ Classical methods error: {str(e)}\")\n",
    "                continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with {wavelet}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE COMPLETE RESULTS\n",
    "# ==============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"💾 SAVING COMPLETE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_rl_results or all_classical_results:\n",
    "    # Combine all results\n",
    "    all_results = all_rl_results + all_classical_results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save main results\n",
    "    main_csv = os.path.join(results_dir, 'out_of_sample', 'complete_results_with_classical_methods_FIXED.csv')\n",
    "    results_df.to_csv(main_csv, index=False)\n",
    "    print(f\"✅ Complete Results saved: {main_csv}\")\n",
    "    \n",
    "    # Save denoising analysis\n",
    "    if denoising_analysis_results:\n",
    "        denoising_df = pd.DataFrame(denoising_analysis_results)\n",
    "        denoising_csv = os.path.join(results_dir, 'denoising_analysis', 'denoising_effectiveness.csv')\n",
    "        denoising_df.to_csv(denoising_csv, index=False)\n",
    "        print(f\"✅ Denoising analysis saved: {denoising_csv}\")\n",
    "    \n",
    "    # Statistical significance analysis\n",
    "    significance_path = os.path.join(results_dir, 'academic_reports', 'statistical_significance.txt')\n",
    "    statistical_significance_analysis(results_df, significance_path)\n",
    "    \n",
    "    # VERIFICAR DIFERENCIAS ENTRE ALGORITMOS\n",
    "    print(f\"\\n🔍 VERIFICANDO RESULTADOS:\")\n",
    "    rl_results = results_df[results_df['Method_Type'] == 'Reinforcement_Learning']\n",
    "    \n",
    "    for algo in ['A2C', 'DQN', 'PPO']:\n",
    "        algo_data = rl_results[rl_results['Algorithm'] == algo]\n",
    "        if len(algo_data) > 0:\n",
    "            best_result = algo_data.loc[algo_data['Sharpe_Ratio'].idxmax()]\n",
    "            print(f\"  {algo}: Sharpe={best_result['Sharpe_Ratio']:.4f}, \"\n",
    "                  f\"Seed={best_result['Random_Seed']}, \"\n",
    "                  f\"Wavelet={best_result['Wavelet']}, \"\n",
    "                  f\"Features={best_result['Features']}\")\n",
    "    \n",
    "    print(f\"\\n✅ CORRECCIONES APLICADAS:\")\n",
    "    print(f\"  ✓ Semillas aleatorias únicas por experimento\")\n",
    "    print(f\"  ✓ Reset completo del environment\")\n",
    "    print(f\"  ✓ Más variación en hiperparámetros A2C/DQN\")\n",
    "    print(f\"  ✓ Normalización mejorada\")\n",
    "    print(f\"  ✓ Registro de semillas en resultados\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results to save. Check for errors in the processing loop.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ COMPLETE ANALYSIS FINISHED!\")\n",
    "print(f\"📁 All results saved in: {results_dir}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
