{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ed4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "from itertools import combinations\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================\n",
    "# SEMILLAS ALEATORIAS MEJORADAS\n",
    "# ==============================================================\n",
    "def set_random_seeds(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    th.manual_seed(seed)\n",
    "    th.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    if hasattr(th.backends, 'cudnn'):\n",
    "        th.backends.cudnn.deterministic = True\n",
    "        th.backends.cudnn.benchmark = False\n",
    "\n",
    "# ==============================================================\n",
    "# CONFIGURATION CON TIMESTEPS REALISTAS Y M√âTRICAS COMPLETAS + M√âTODOS CL√ÅSICOS\n",
    "# ==============================================================\n",
    "POINT_VALUE = 50\n",
    "COMMISSION_PER_TRADE = 0.25\n",
    "SPREAD = 0.25\n",
    "\n",
    "# üöÄ WAVELETS EXPANDIDAS (para responder a Reviewer #2)\n",
    "wavelet_families = {\n",
    "    'Daubechies': ['db1', 'db2', 'db4'],      \n",
    "    'Symlets': ['sym1', 'sym4'],        \n",
    "    'Coiflets': ['coif1', 'coif4'],\n",
    "    'Biorthogonal': ['bior1.1','bior2.2']\n",
    "}\n",
    "\n",
    "# üöÄ HYPERPARAMETERS REALISTAS - CORREGIDO CON M√ÅS VARIACI√ìN\n",
    "HYPERPARAM_GRID = {\n",
    "    'PPO': {\n",
    "        'learning_rate': [3e-4, 1e-4],     \n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'n_steps': [2048],                 \n",
    "        'batch_size': [128]                \n",
    "    },\n",
    "    'A2C': {\n",
    "        'learning_rate': [1e-4, 5e-5, 3e-4],    # M√ÅS VARIACI√ìN\n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'n_steps': [20, 50]                      # M√ÅS VARIACI√ìN\n",
    "    },\n",
    "    'DQN': {\n",
    "        'learning_rate': [1e-4, 5e-5, 3e-4],    # M√ÅS VARIACI√ìN\n",
    "        'gamma': [0.95, 0.99],             \n",
    "        'buffer_size': [100000, 50000],          # M√ÅS VARIACI√ìN\n",
    "        'exploration_fraction': [0.1, 0.2]      # M√ÅS VARIACI√ìN\n",
    "    }\n",
    "}\n",
    "\n",
    "# üöÄ TIMESTEPS REALISTAS\n",
    "TRAINING_TIMESTEPS = {\n",
    "    'hyperparameter_tuning': 10000,    \n",
    "    'final_training': 50000,           \n",
    "    'validation_steps': 5000           \n",
    "}\n",
    "\n",
    "# üöÄ TODAS LAS COMBINACIONES DE FEATURES\n",
    "MAIN_FEATURE_COMBINATIONS = [\n",
    "    ('DIX',), ('GEX',), ('VIX',),\n",
    "    ('DIX', 'GEX'), ('DIX', 'VIX'), ('GEX', 'VIX'),\n",
    "    ('DIX', 'GEX', 'VIX')\n",
    "]\n",
    "\n",
    "# Paths\n",
    "file_path = r\"C:\\Users\\elect\\OneDrive\\Documentos\\Doctorado\\Articulo 4 peer review\\Data\\merged_data_with_vix_2.csv\"\n",
    "results_dir = r\"C:\\Users\\elect\\OneDrive\\Documentos\\Doctorado\\Articulo 4 peer review\\results_COMPLETE_WITH_CLASSICAL_METHODS\"\n",
    "\n",
    "# Create directories\n",
    "directories = ['graphs', 'csv', 'out_of_sample', 'academic_reports', 'denoising_analysis', 'wavelet_analysis']\n",
    "for dir_name in directories:\n",
    "    os.makedirs(os.path.join(results_dir, dir_name), exist_ok=True)\n",
    "\n",
    "print(\"üöÄ COMPLETE CONFIGURATION WITH ALL IMPROVEMENTS FOR REVIEWERS:\")\n",
    "print(f\"üìä Wavelet families: {len(wavelet_families)} ({sum(len(w) for w in wavelet_families.values())} wavelets)\")\n",
    "print(f\"ü§ñ RL Algorithms: PPO, A2C, DQN\")  \n",
    "print(f\"üîß Classical Methods: XGBoost, Random Forest, Logistic Regression, Buy&Hold, Moving Average\")\n",
    "print(f\"üîß Feature combinations: {len(MAIN_FEATURE_COMBINATIONS)}\")\n",
    "print(f\"üß† Feature Extractor: ADVANCED (Attention + BiLSTM + Positional Encoding)\")\n",
    "print(f\"üìà Strategy: LONG-ONLY (Buy/Hold - No Short Selling)\")\n",
    "print(f\"üéØ Training timesteps: {TRAINING_TIMESTEPS['final_training']:,}\")\n",
    "print(f\"üìä Metrics: Alpha, Beta, Sharpe, Sortino, Max DD, Win Rate, Max Loss, RoMaD, CAGR, Volatility, T-Stat, P-Value\")\n",
    "print(f\"üî¨ Analysis: Denoising effectiveness, Wavelet level justification, Statistical significance\")\n",
    "\n",
    "# ==============================================================\n",
    "# DATA LOADING\n",
    "# ==============================================================\n",
    "print(\"\\nüìä Loading data...\")\n",
    "data = pd.read_csv(file_path)\n",
    "print(f\"üìã Dataset Description:\")\n",
    "print(f\"   ‚Ä¢ Shape: {data.shape}\")\n",
    "print(f\"   ‚Ä¢ Columns: {list(data.columns)}\")\n",
    "print(f\"   ‚Ä¢ Date range: {data['date'].min()} to {data['date'].max()}\")\n",
    "print(f\"   ‚Ä¢ Missing values: {data.isnull().sum().sum()}\")\n",
    "print(f\"   ‚Ä¢ Description: S&P 500 futures data with DIX, GEX, VIX indicators for algorithmic trading\")\n",
    "\n",
    "data.set_index('date', inplace=True)\n",
    "print(f\"‚úÖ Data loaded and indexed by date\")\n",
    "\n",
    "def split_data_temporal(data, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_data = data.iloc[:train_end].copy()\n",
    "    val_data = data.iloc[train_end:val_end].copy()\n",
    "    test_data = data.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"üìà Temporal Split (No Data Leakage):\")\n",
    "    print(f\"   Train: {len(train_data)} samples ({train_data.index[0]} to {train_data.index[-1]})\")\n",
    "    print(f\"   Val: {len(val_data)} samples ({val_data.index[0]} to {val_data.index[-1]})\")\n",
    "    print(f\"   Test: {len(test_data)} samples ({test_data.index[0]} to {test_data.index[-1]})\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# ==============================================================\n",
    "# TRADING ENVIRONMENT - LONG ONLY - CORREGIDO CON RESET COMPLETO\n",
    "# ==============================================================\n",
    "class SimpleTradingEnv(gym.Env):\n",
    "    def __init__(self, indicator_data, price_data):\n",
    "        super(SimpleTradingEnv, self).__init__()\n",
    "        \n",
    "        self.indicator_data = np.array(indicator_data, dtype=np.float32)\n",
    "        self.price_data = np.array(price_data, dtype=np.float32)\n",
    "        \n",
    "        # Ensure same length\n",
    "        min_len = min(len(self.indicator_data), len(self.price_data))\n",
    "        self.indicator_data = self.indicator_data[:min_len]\n",
    "        self.price_data = self.price_data[:min_len]\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.initial_balance = 100000\n",
    "        self.reset()\n",
    "        \n",
    "        # Observation space\n",
    "        if len(self.indicator_data.shape) == 1:\n",
    "            obs_shape = (1,)\n",
    "        else:\n",
    "            obs_shape = (self.indicator_data.shape[1],)\n",
    "            \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-10.0, high=10.0, shape=obs_shape, dtype=np.float32\n",
    "        )\n",
    "        # ONLY 2 ACTIONS: 0: Hold/Sell, 1: Buy\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "    def reset(self):\n",
    "        # RESET COMPLETO de todas las variables\n",
    "        self.current_step = 0\n",
    "        self.balance = float(self.initial_balance)  # Ensure float\n",
    "        self.position = 0\n",
    "        self.entry_price = 0.0  # Ensure float\n",
    "        self.balance_history = [float(self.initial_balance)]\n",
    "        self.trades = 0\n",
    "        \n",
    "        # IMPORTANTE: Resetear tambi√©n el estado interno\n",
    "        self._last_action = None\n",
    "        self._episode_steps = 0\n",
    "        \n",
    "        if len(self.indicator_data.shape) == 1:\n",
    "            return np.array([self.indicator_data[0]], dtype=np.float32)\n",
    "        else:\n",
    "            return self.indicator_data[0].astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.price_data) - 1:\n",
    "            # Close any open position at the end\n",
    "            if self.position == 1:\n",
    "                current_price = self.price_data[self.current_step]\n",
    "                # Sell position (incluye spread y comisiones)\n",
    "                pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "                self.balance += pnl\n",
    "                self.position = 0\n",
    "                self.trades += 1\n",
    "            \n",
    "            self.balance_history.append(self.balance)\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "            return obs, 0, True, {\"balance\": self.balance, \"trades\": self.trades}\n",
    "        \n",
    "        current_price = self.price_data[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # LONG ONLY ACTION LOGIC\n",
    "        if action == 1:  # BUY\n",
    "            if self.position == 0:  # No position -> Open long\n",
    "                self.position = 1\n",
    "                self.entry_price = current_price + SPREAD  # Include spread\n",
    "                self.balance -= COMMISSION_PER_TRADE\n",
    "                reward -= COMMISSION_PER_TRADE / 1000\n",
    "                self.trades += 1\n",
    "                \n",
    "        elif action == 0:  # HOLD/SELL\n",
    "            if self.position == 1:  # Long position -> Close long\n",
    "                pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "                self.balance += pnl\n",
    "                reward += pnl / 1000\n",
    "                self.position = 0\n",
    "                self.trades += 1\n",
    "        \n",
    "        # Calculate current balance (with unrealized PnL if in position)\n",
    "        current_balance = self.balance\n",
    "        if self.position == 1:\n",
    "            unrealized_pnl = (current_price - self.entry_price - SPREAD) * POINT_VALUE\n",
    "            current_balance += unrealized_pnl\n",
    "        \n",
    "        self.balance_history.append(current_balance)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.price_data) - 1\n",
    "        \n",
    "        # Next observation\n",
    "        if not done:\n",
    "            if len(self.indicator_data.shape) == 1:\n",
    "                obs = np.array([self.indicator_data[self.current_step]], dtype=np.float32)\n",
    "            else:\n",
    "                obs = self.indicator_data[self.current_step].astype(np.float32)\n",
    "        else:\n",
    "            obs = np.zeros(self.observation_space.shape, dtype=np.float32)\n",
    "        \n",
    "        info = {\"balance\": current_balance, \"trades\": self.trades, \"position\": self.position}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def get_balance_history(self):\n",
    "        return self.balance_history.copy()\n",
    "\n",
    "# ==============================================================\n",
    "# üß† ADVANCED FEATURE EXTRACTOR\n",
    "# ==============================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = th.arange(max_len).unsqueeze(1)\n",
    "        div_term = th.exp(th.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = th.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = th.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = th.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = th.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AdvancedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, observation_space, features_dim=64, n_heads=4, dropout=0.1):\n",
    "        super(AdvancedFeatureExtractor, self).__init__()\n",
    "        self.features_dim = features_dim\n",
    "        input_dim = observation_space.shape[0]\n",
    "        \n",
    "        print(f\"üß† Creating AdvancedFeatureExtractor with input_dim={input_dim}, features_dim={features_dim}\")\n",
    "        \n",
    "        hidden_dim = min(128, max(64, input_dim * 16))\n",
    "        \n",
    "        self.ff_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        attention_dim = hidden_dim // 2\n",
    "        self.attention = MultiheadAttention(\n",
    "            embed_dim=attention_dim, \n",
    "            num_heads=min(n_heads, attention_dim // 16),\n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attention_norm = nn.LayerNorm(attention_dim)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            attention_dim, \n",
    "            attention_dim // 2, \n",
    "            bidirectional=True, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if dropout > 0 else 0\n",
    "        )\n",
    "        self.lstm_norm = nn.LayerNorm(attention_dim)\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(attention_dim, features_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(features_dim * 2, features_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(attention_dim, dropout=dropout)\n",
    "        \n",
    "        print(f\"‚úÖ AdvancedFeatureExtractor created successfully!\")\n",
    "        \n",
    "    def forward(self, observations):\n",
    "        original_shape = observations.shape\n",
    "        if len(original_shape) == 1:\n",
    "            observations = observations.unsqueeze(0)\n",
    "        \n",
    "        x = self.ff_encoder(observations)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        x_attn, _ = self.attention(x, x, x)\n",
    "        x = self.attention_norm(x + x_attn)\n",
    "        \n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        x = self.lstm_norm(lstm_out)\n",
    "        \n",
    "        x = x.squeeze(1)\n",
    "        features = self.projection(x)\n",
    "        \n",
    "        if len(original_shape) == 1:\n",
    "            features = features.squeeze(0)\n",
    "            \n",
    "        return features\n",
    "\n",
    "# ==============================================================\n",
    "# üìä COMPLETE METRICS CALCULATION\n",
    "# ==============================================================\n",
    "def calculate_complete_metrics(balance_history, price_data, benchmark_returns=None):\n",
    "    \"\"\"Calculate ALL requested metrics including Alpha, Beta, Sortino, etc.\"\"\"\n",
    "    \n",
    "    if len(balance_history) < 2:\n",
    "        return {\n",
    "            \"Alpha\": 0.0, \"Beta\": 0.0, \"Sharpe_Ratio\": 0.0, \"Sortino_Ratio\": 0.0,\n",
    "            \"Max_Drawdown\": 0.0, \"Win_Rate\": 0.0, \"Max_Loss\": 0.0, \"RoMaD\": 0.0,\n",
    "            \"CAGR\": 0.0, \"Volatility\": 0.0, \"T_Statistic\": 0.0, \"P_Value\": 1.0,\n",
    "            \"Total_Return\": 0.0\n",
    "        }\n",
    "    \n",
    "    balance_history = np.array(balance_history)\n",
    "    returns = np.diff(balance_history) / balance_history[:-1]\n",
    "    returns = returns[np.isfinite(returns)]\n",
    "    \n",
    "    if len(returns) < 2:\n",
    "        return {\n",
    "            \"Alpha\": 0.0, \"Beta\": 0.0, \"Sharpe_Ratio\": 0.0, \"Sortino_Ratio\": 0.0,\n",
    "            \"Max_Drawdown\": 0.0, \"Win_Rate\": 0.0, \"Max_Loss\": 0.0, \"RoMaD\": 0.0,\n",
    "            \"CAGR\": 0.0, \"Volatility\": 0.0, \"T_Statistic\": 0.0, \"P_Value\": 1.0,\n",
    "            \"Total_Return\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_return = (balance_history[-1] - balance_history[0]) / balance_history[0]\n",
    "    \n",
    "    # CAGR\n",
    "    n_years = len(balance_history) / 252\n",
    "    if n_years > 0 and balance_history[0] > 0:\n",
    "        cagr = (balance_history[-1] / balance_history[0]) ** (1 / n_years) - 1\n",
    "    else:\n",
    "        cagr = 0\n",
    "    \n",
    "    # Volatility (Annualized)\n",
    "    volatility = np.std(returns) * np.sqrt(252) if len(returns) > 1 else 0\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    risk_free_rate = 0.02\n",
    "    excess_returns = returns - (risk_free_rate / 252)\n",
    "    sharpe_ratio = np.mean(excess_returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
    "    \n",
    "    # Drawdown metrics\n",
    "    peak = np.maximum.accumulate(balance_history)\n",
    "    drawdown = (peak - balance_history) / np.maximum(peak, 1)\n",
    "    max_drawdown = np.max(drawdown)\n",
    "    \n",
    "    # Sortino Ratio\n",
    "    negative_returns = returns[returns < 0]\n",
    "    downside_std = np.std(negative_returns) * np.sqrt(252) if len(negative_returns) > 1 else 0.001\n",
    "    sortino_ratio = np.mean(excess_returns) * np.sqrt(252) / downside_std if downside_std > 0 else 0\n",
    "    \n",
    "    # Win Rate\n",
    "    positive_returns = returns[returns > 0]\n",
    "    win_rate = len(positive_returns) / len(returns) if len(returns) > 0 else 0\n",
    "    \n",
    "    # Max Loss\n",
    "    max_loss = np.min(returns) if len(returns) > 0 else 0\n",
    "    \n",
    "    # RoMaD\n",
    "    romad = cagr / max_drawdown if max_drawdown > 0 else 0\n",
    "    \n",
    "    # T-statistic and P-value\n",
    "    if len(returns) > 1:\n",
    "        t_stat, p_value = stats.ttest_1samp(returns, 0)\n",
    "        t_stat = float(t_stat) if not np.isnan(t_stat) else 0\n",
    "        p_value = float(p_value) if not np.isnan(p_value) else 1\n",
    "    else:\n",
    "        t_stat, p_value = 0, 1\n",
    "    \n",
    "    # Alpha and Beta\n",
    "    if benchmark_returns is not None and len(benchmark_returns) == len(returns):\n",
    "        try:\n",
    "            covariance = np.cov(returns, benchmark_returns)[0][1]\n",
    "            benchmark_variance = np.var(benchmark_returns)\n",
    "            beta = covariance / benchmark_variance if benchmark_variance > 0 else 0\n",
    "            \n",
    "            benchmark_mean = np.mean(benchmark_returns)\n",
    "            returns_mean = np.mean(returns)\n",
    "            alpha = returns_mean - (risk_free_rate/252 + beta * (benchmark_mean - risk_free_rate/252))\n",
    "            alpha = alpha * 252  # Annualize\n",
    "            \n",
    "        except:\n",
    "            alpha, beta = 0, 0\n",
    "    else:\n",
    "        market_return = 0.08 / 252\n",
    "        market_returns = np.full(len(returns), market_return)\n",
    "        \n",
    "        try:\n",
    "            covariance = np.cov(returns, market_returns)[0][1]\n",
    "            market_variance = np.var(market_returns)\n",
    "            beta = covariance / market_variance if market_variance > 0 else 0\n",
    "            \n",
    "            returns_mean = np.mean(returns)\n",
    "            alpha = returns_mean - (risk_free_rate/252 + beta * (market_return - risk_free_rate/252))\n",
    "            alpha = alpha * 252\n",
    "        except:\n",
    "            alpha, beta = 0, 0\n",
    "    \n",
    "    return {\n",
    "        \"Alpha\": float(alpha),\n",
    "        \"Beta\": float(beta),\n",
    "        \"Sharpe_Ratio\": float(sharpe_ratio),\n",
    "        \"Sortino_Ratio\": float(sortino_ratio),\n",
    "        \"Max_Drawdown\": float(max_drawdown),\n",
    "        \"Win_Rate\": float(win_rate),\n",
    "        \"Max_Loss\": float(max_loss),\n",
    "        \"RoMaD\": float(romad),\n",
    "        \"CAGR\": float(cagr),\n",
    "        \"Volatility\": float(volatility),\n",
    "        \"T_Statistic\": float(t_stat),\n",
    "        \"P_Value\": float(p_value),\n",
    "        \"Total_Return\": float(total_return)\n",
    "    }\n",
    "\n",
    "# ==============================================================\n",
    "# üî¨ DENOISING ANALYSIS\n",
    "# ==============================================================\n",
    "def analyze_denoising_effectiveness(original_data, denoised_data, wavelet_name, save_path):\n",
    "    \"\"\"Analyze and visualize denoising effectiveness\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original vs Denoised\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(original_data[:200], label='Original', alpha=0.7, linewidth=1)\n",
    "    plt.plot(denoised_data[:200], label='Denoised', alpha=0.8, linewidth=1.5)\n",
    "    plt.title(f'Signal Comparison - {wavelet_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Noise removed\n",
    "    plt.subplot(2, 3, 2)\n",
    "    noise = original_data - denoised_data\n",
    "    plt.plot(noise[:200], color='red', alpha=0.7)\n",
    "    plt.title('Removed Noise')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # FFT Analysis\n",
    "    plt.subplot(2, 3, 3)\n",
    "    freqs_orig = np.fft.fftfreq(len(original_data))\n",
    "    fft_orig = np.abs(np.fft.fft(original_data))\n",
    "    fft_clean = np.abs(np.fft.fft(denoised_data))\n",
    "    plt.semilogy(freqs_orig[:len(freqs_orig)//2], fft_orig[:len(fft_orig)//2], alpha=0.7, label='Original')\n",
    "    plt.semilogy(freqs_orig[:len(freqs_orig)//2], fft_clean[:len(fft_clean)//2], alpha=0.8, label='Denoised')\n",
    "    plt.title('Frequency Domain Analysis')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Wavelet decomposition visualization\n",
    "    plt.subplot(2, 3, 4)\n",
    "    coeffs = pywt.wavedec(original_data, wavelet_name, level=2)\n",
    "    plt.plot(coeffs[0][:100], label='Approximation', linewidth=1.5)\n",
    "    plt.title('Approximation Coefficients (L2)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(coeffs[1][:50], label='Detail L1', alpha=0.8)\n",
    "    plt.plot(coeffs[2][:50], label='Detail L2', alpha=0.8)\n",
    "    plt.title('Detail Coefficients')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Metrics calculation\n",
    "    signal_power = np.var(denoised_data)\n",
    "    noise_power = np.var(noise)\n",
    "    snr = 10 * np.log10(signal_power / noise_power) if noise_power > 0 else float('inf')\n",
    "    noise_reduction = (np.std(noise)/np.std(original_data)*100)\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.text(0.1, 0.8, f'SNR Improvement: {snr:.2f} dB', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.7, f'Noise Reduction: {noise_reduction:.1f}%', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.6, f'Wavelet: {wavelet_name}', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.5, f'Decomposition Level: 2', fontsize=12, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.4, f'Threshold: Conservative (0.1 * œÉ)', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.3, f'Method: Soft thresholding', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.2, f'Justification: Level 2 provides good', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.text(0.1, 0.1, f'balance between denoising and signal preservation', fontsize=10, transform=plt.gca().transAxes)\n",
    "    plt.axis('off')\n",
    "    plt.title('Denoising Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return snr, noise_power, noise_reduction\n",
    "\n",
    "# ==============================================================\n",
    "# üìä STATISTICAL SIGNIFICANCE ANALYSIS\n",
    "# ==============================================================\n",
    "def statistical_significance_analysis(results_df, save_path):\n",
    "    \"\"\"Perform statistical significance tests\"\"\"\n",
    "    \n",
    "    algorithms = results_df['Algorithm'].unique()\n",
    "    metrics = ['Sharpe_Ratio', 'Total_Return', 'Max_Drawdown', 'Alpha', 'Sortino_Ratio']\n",
    "    \n",
    "    significance_results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        significance_results[metric] = {}\n",
    "        for i, algo1 in enumerate(algorithms):\n",
    "            for algo2 in algorithms[i+1:]:\n",
    "                data1 = results_df[results_df['Algorithm'] == algo1][metric]\n",
    "                data2 = results_df[results_df['Algorithm'] == algo2][metric]\n",
    "                \n",
    "                if len(data1) > 1 and len(data2) > 1:\n",
    "                    t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "                    significance_results[metric][f'{algo1}_vs_{algo2}'] = {\n",
    "                        't_statistic': t_stat,\n",
    "                        'p_value': p_value,\n",
    "                        'significant': p_value < 0.05,\n",
    "                        'mean_diff': float(data1.mean() - data2.mean())\n",
    "                    }\n",
    "    \n",
    "    # Create significance table and save\n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(\"STATISTICAL SIGNIFICANCE ANALYSIS\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for metric in metrics:\n",
    "            f.write(f\"\\n{metric.upper()}:\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\n\")\n",
    "            for comparison, stats_result in significance_results[metric].items():\n",
    "                significance = \"SIGNIFICANT\" if stats_result['significant'] else \"NOT SIGNIFICANT\"\n",
    "                f.write(f\"{comparison}: p={stats_result['p_value']:.4f} ({significance})\\n\")\n",
    "                f.write(f\"  Mean difference: {stats_result['mean_diff']:.4f}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Statistical significance analysis saved to: {save_path}\")\n",
    "    return significance_results\n",
    "\n",
    "# ==============================================================\n",
    "# ü§ñ CLASSICAL METHODS IMPLEMENTATION\n",
    "# ==============================================================\n",
    "def implement_classical_methods(train_features, train_labels, test_features, test_prices):\n",
    "    \"\"\"Implement classical ML methods including XGBoost\"\"\"\n",
    "    \n",
    "    classical_results = {}\n",
    "    \n",
    "    # 1. XGBoost\n",
    "    print(\"  üöÄ Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(train_features, train_labels)\n",
    "    xgb_predictions = xgb_model.predict(test_features)\n",
    "    xgb_balance = simulate_trading_strategy(xgb_predictions, test_prices)\n",
    "    classical_results['XGBoost'] = calculate_complete_metrics(xgb_balance, test_prices)\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"  üå≤ Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(train_features, train_labels)\n",
    "    rf_predictions = rf_model.predict(test_features)\n",
    "    rf_balance = simulate_trading_strategy(rf_predictions, test_prices)\n",
    "    classical_results['Random_Forest'] = calculate_complete_metrics(rf_balance, test_prices)\n",
    "    \n",
    "    # 3. Logistic Regression\n",
    "    print(\"  üìä Training Logistic Regression...\")\n",
    "    lr_model = LogisticRegression(random_state=42)\n",
    "    lr_model.fit(train_features, train_labels)\n",
    "    lr_predictions = lr_model.predict(test_features)\n",
    "    lr_balance = simulate_trading_strategy(lr_predictions, test_prices)\n",
    "    classical_results['Logistic_Regression'] = calculate_complete_metrics(lr_balance, test_prices)\n",
    "    \n",
    "    # 4. Buy and Hold\n",
    "    print(\"  üí∞ Calculating Buy & Hold...\")\n",
    "    buy_hold_return = (test_prices[-1] - test_prices[0]) / test_prices[0]\n",
    "    buy_hold_balance = [100000 * (1 + (price - test_prices[0]) / test_prices[0]) for price in test_prices]\n",
    "    classical_results['Buy_and_Hold'] = calculate_complete_metrics(buy_hold_balance, test_prices)\n",
    "    \n",
    "    # 5. Moving Average Crossover\n",
    "    print(\"  üìà Calculating Moving Average Strategy...\")\n",
    "    ma_short = pd.Series(test_prices).rolling(20).mean()\n",
    "    ma_long = pd.Series(test_prices).rolling(50).mean()\n",
    "    ma_signals = np.where(ma_short > ma_long, 1, 0)\n",
    "    ma_balance = simulate_trading_strategy(ma_signals, test_prices)\n",
    "    classical_results['Moving_Average'] = calculate_complete_metrics(ma_balance, test_prices)\n",
    "    \n",
    "    return classical_results\n",
    "\n",
    "def create_trading_labels(price_data, lookforward=5):\n",
    "    \"\"\"Create trading labels for supervised learning\"\"\"\n",
    "    labels = []\n",
    "    for i in range(len(price_data) - lookforward):\n",
    "        future_return = (price_data[i + lookforward] - price_data[i]) / price_data[i]\n",
    "        labels.append(1 if future_return > 0.002 else 0)  # 0.2% threshold\n",
    "    \n",
    "    return np.array(labels)\n",
    "\n",
    "def simulate_trading_strategy(signals, prices):\n",
    "    \"\"\"Simulate trading strategy given signals and prices\"\"\"\n",
    "    balance = 100000\n",
    "    position = 0\n",
    "    balance_history = [balance]\n",
    "    \n",
    "    for i, signal in enumerate(signals):\n",
    "        current_price = prices[i]\n",
    "        \n",
    "        if signal == 1 and position == 0:  # Buy signal\n",
    "            position = 1\n",
    "            entry_price = current_price + SPREAD\n",
    "            balance -= COMMISSION_PER_TRADE\n",
    "        elif signal == 0 and position == 1:  # Sell signal\n",
    "            pnl = (current_price - entry_price - SPREAD) * POINT_VALUE - COMMISSION_PER_TRADE\n",
    "            balance += pnl\n",
    "            position = 0\n",
    "        \n",
    "        # Calculate current balance\n",
    "        current_balance = balance\n",
    "        if position == 1:\n",
    "            unrealized_pnl = (current_price - entry_price - SPREAD) * POINT_VALUE\n",
    "            current_balance += unrealized_pnl\n",
    "        \n",
    "        balance_history.append(current_balance)\n",
    "    \n",
    "    return balance_history\n",
    "\n",
    "# ==============================================================\n",
    "# WAVELET PROCESSING\n",
    "# ==============================================================\n",
    "def simple_wavelet_denoise(series, wavelet):\n",
    "    \"\"\"Simplified wavelet denoising with analysis\"\"\"\n",
    "    try:\n",
    "        coeffs = pywt.wavedec(series, wavelet, level=2)\n",
    "        threshold = np.std(coeffs[-1]) * 0.1\n",
    "        coeffs[1:] = [pywt.threshold(c, threshold, 'soft') for c in coeffs[1:]]\n",
    "        return pywt.waverec(coeffs, wavelet)[:len(series)]\n",
    "    except:\n",
    "        return series\n",
    "\n",
    "# ==============================================================\n",
    "# HYPERPARAMETER TUNING - CORREGIDO CON M√ÅS COMBINACIONES\n",
    "# ==============================================================\n",
    "def realistic_hyperparameter_tuning(algo_class, train_features, val_features, train_prices, val_prices):\n",
    "    \"\"\"Hyperparameter tuning with realistic timesteps\"\"\"\n",
    "    \n",
    "    param_grid = HYPERPARAM_GRID[algo_class.__name__]\n",
    "    best_model = None\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    \n",
    "    # CORREGIDO: Generar m√°s combinaciones para A2C y DQN\n",
    "    if algo_class.__name__ == 'PPO':\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][0], 'batch_size': param_grid['batch_size'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'n_steps': param_grid['n_steps'][0], 'batch_size': param_grid['batch_size'][0]}\n",
    "        ]\n",
    "    elif algo_class.__name__ == 'A2C':\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'n_steps': param_grid['n_steps'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][2], 'gamma': param_grid['gamma'][0], \n",
    "             'n_steps': param_grid['n_steps'][1]},  # Nueva combinaci√≥n\n",
    "        ]\n",
    "    else:  # DQN\n",
    "        combinations = [\n",
    "            {'learning_rate': param_grid['learning_rate'][0], 'gamma': param_grid['gamma'][0], \n",
    "             'buffer_size': param_grid['buffer_size'][0], 'exploration_fraction': param_grid['exploration_fraction'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][1], 'gamma': param_grid['gamma'][1], \n",
    "             'buffer_size': param_grid['buffer_size'][0], 'exploration_fraction': param_grid['exploration_fraction'][0]},\n",
    "            {'learning_rate': param_grid['learning_rate'][2], 'gamma': param_grid['gamma'][0], \n",
    "             'buffer_size': param_grid['buffer_size'][1], 'exploration_fraction': param_grid['exploration_fraction'][1]},  # Nueva combinaci√≥n\n",
    "        ]\n",
    "    \n",
    "    print(f\"üîç Testing {len(combinations)} combinations with {TRAINING_TIMESTEPS['hyperparameter_tuning']:,} timesteps\")\n",
    "    \n",
    "    for i, params in enumerate(combinations):\n",
    "        print(f\"      ‚öôÔ∏è {i+1}/{len(combinations)}: {params}\")\n",
    "        \n",
    "        try:\n",
    "            train_env = DummyVecEnv([lambda: SimpleTradingEnv(train_features.copy(), train_prices.copy())])\n",
    "            train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "            \n",
    "            policy_kwargs = dict(\n",
    "                features_extractor_class=AdvancedFeatureExtractor,\n",
    "                features_extractor_kwargs=dict(features_dim=64, n_heads=4, dropout=0.1)\n",
    "            )\n",
    "            \n",
    "            model = algo_class('MlpPolicy', train_env, verbose=0, policy_kwargs=policy_kwargs, **params)\n",
    "            model.learn(total_timesteps=TRAINING_TIMESTEPS['hyperparameter_tuning'])\n",
    "            \n",
    "            # Validation\n",
    "            val_env = DummyVecEnv([lambda: SimpleTradingEnv(val_features.copy(), val_prices.copy())])\n",
    "            val_env = VecNormalize(val_env, norm_obs=True, norm_reward=False, training=False)\n",
    "            # CORREGIDO: Copiar estad√≠sticas de normalizaci√≥n (no solo referencia)\n",
    "            val_env.obs_rms = train_env.obs_rms.copy()\n",
    "            \n",
    "            obs = val_env.reset()\n",
    "            val_balance = [100000]\n",
    "            done = [False]\n",
    "            \n",
    "            while not done[0]:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, _, done, info = val_env.step(action)\n",
    "                val_balance.append(info[0]['balance'])\n",
    "            \n",
    "            val_metrics = calculate_complete_metrics(val_balance, val_prices)\n",
    "            score = val_metrics['Sharpe_Ratio'] + val_metrics['Sortino_Ratio'] + val_metrics['Total_Return'] - val_metrics['Max_Drawdown']\n",
    "            \n",
    "            print(f\"        üìä Score: {score:.3f}\")\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "                best_params = params.copy()\n",
    "                best_params['train_env'] = train_env\n",
    "                best_params['score'] = score\n",
    "            \n",
    "            val_env.close()\n",
    "            if best_params.get('train_env') != train_env:\n",
    "                train_env.close()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"        ‚ùå Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return best_model, best_params\n",
    "\n",
    "# ==============================================================\n",
    "# MAIN PROCESSING LOOP - CORREGIDO CON SEMILLAS √öNICAS\n",
    "# ==============================================================\n",
    "print(\"\\nüöÄ Starting COMPLETE ANALYSIS with ALL IMPROVEMENTS...\")\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = split_data_temporal(data)\n",
    "benchmark_returns = np.diff(test_data['PRICE'].values) / test_data['PRICE'].values[:-1]\n",
    "\n",
    "# Initialize results\n",
    "all_rl_results = []\n",
    "all_classical_results = []\n",
    "denoising_analysis_results = []\n",
    "\n",
    "total_experiments = sum(len(wavelets) for wavelets in wavelet_families.values()) * 3 * len(MAIN_FEATURE_COMBINATIONS)\n",
    "current_experiment = 0\n",
    "\n",
    "print(f\"\\nüìä TOTAL EXPERIMENTS TO RUN: {total_experiments}\")\n",
    "print(f\"‚è±Ô∏è Estimated time: ~{total_experiments * 8 / 60:.1f} hours\")\n",
    "\n",
    "# Process each family\n",
    "for family_name, wavelets in wavelet_families.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üåä PROCESSING: {family_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for wavelet in wavelets:\n",
    "        print(f\"\\nüì° Wavelet: {wavelet}\")\n",
    "        \n",
    "        try:\n",
    "            # Process all datasets\n",
    "            processed_data = {}\n",
    "            \n",
    "            for dataset_name, dataset in [('train', train_data), ('val', val_data), ('test', test_data)]:\n",
    "                price_data = dataset['PRICE'].values\n",
    "                indicator_data = dataset[['DIX', 'GEX', 'VIX']].values\n",
    "                \n",
    "                # Denoising with analysis\n",
    "                denoised_data = []\n",
    "                for col_idx, col_name in enumerate(['DIX', 'GEX', 'VIX']):\n",
    "                    series = indicator_data[:, col_idx]\n",
    "                    if np.any(np.isnan(series)):\n",
    "                        series = pd.Series(series).fillna(method='ffill').fillna(0).values\n",
    "                    \n",
    "                    denoised = simple_wavelet_denoise(series, wavelet)\n",
    "                    denoised_data.append(denoised)\n",
    "                    \n",
    "                    # Analyze denoising effectiveness for test data only\n",
    "                    if dataset_name == 'test':\n",
    "                        denoising_save_path = os.path.join(results_dir, 'denoising_analysis', \n",
    "                                                         f'denoising_{wavelet}_{col_name}.png')\n",
    "                        snr, noise_power, noise_reduction = analyze_denoising_effectiveness(\n",
    "                            series, denoised, wavelet, denoising_save_path)\n",
    "                        \n",
    "                        denoising_analysis_results.append({\n",
    "                            'Wavelet': wavelet,\n",
    "                            'Indicator': col_name,\n",
    "                            'SNR_Improvement': snr,\n",
    "                            'Noise_Reduction_Percent': noise_reduction,\n",
    "                            'Noise_Power': noise_power\n",
    "                        })\n",
    "                \n",
    "                denoised_data = np.column_stack(denoised_data)\n",
    "                \n",
    "                # Normalization\n",
    "                if dataset_name == 'train':\n",
    "                    scaler = MinMaxScaler()\n",
    "                    normalized_data = scaler.fit_transform(denoised_data)\n",
    "                    processed_data['scaler'] = scaler\n",
    "                else:\n",
    "                    normalized_data = processed_data['scaler'].transform(denoised_data)\n",
    "                \n",
    "                processed_data[dataset_name] = {\n",
    "                    'price': price_data,\n",
    "                    'indicators': normalized_data\n",
    "                }\n",
    "            \n",
    "            # Feature mapping\n",
    "            feature_mapping = {\n",
    "                'DIX': [0], 'GEX': [1], 'VIX': [2],\n",
    "                'DIX_GEX': [0, 1], 'DIX_VIX': [0, 2], 'GEX_VIX': [1, 2],\n",
    "                'DIX_GEX_VIX': [0, 1, 2]\n",
    "            }\n",
    "            \n",
    "            # Process RL algorithms\n",
    "            algorithms = {'PPO': PPO, 'A2C': A2C, 'DQN': DQN}\n",
    "            \n",
    "            for algo_name, algo_class in algorithms.items():\n",
    "                print(f\"\\n  ü§ñ {algo_name}\")\n",
    "                \n",
    "                for feature_comb in MAIN_FEATURE_COMBINATIONS:\n",
    "                    current_experiment += 1\n",
    "                    \n",
    "                    if len(feature_comb) == 1:\n",
    "                        feature_name = feature_comb[0]\n",
    "                    else:\n",
    "                        feature_name = '_'.join(feature_comb)\n",
    "                    \n",
    "                    feature_indices = feature_mapping[feature_name]\n",
    "                    \n",
    "                    # CORREGIDO: Semilla √∫nica por algoritmo, wavelet y features\n",
    "                    algo_seed = hash(f\"{algo_name}_{wavelet}_{feature_name}_{current_experiment}\") % (2**32)\n",
    "                    set_random_seeds(algo_seed)\n",
    "                    \n",
    "                    print(f\"    üìä [{current_experiment}/{total_experiments}] {feature_name} (seed: {algo_seed})\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Extract features\n",
    "                        train_features = processed_data['train']['indicators'][:, feature_indices]\n",
    "                        val_features = processed_data['val']['indicators'][:, feature_indices]\n",
    "                        test_features = processed_data['test']['indicators'][:, feature_indices]\n",
    "                        \n",
    "                        # Hyperparameter tuning\n",
    "                        best_model, best_params = realistic_hyperparameter_tuning(\n",
    "                            algo_class, train_features, val_features,\n",
    "                            processed_data['train']['price'], processed_data['val']['price']\n",
    "                        )\n",
    "                        \n",
    "                        if best_model is None:\n",
    "                            print(f\"      ‚ùå No valid model\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Final training\n",
    "                        print(f\"      üéØ Final training with {TRAINING_TIMESTEPS['final_training']:,} timesteps...\")\n",
    "                        train_env = DummyVecEnv([lambda: SimpleTradingEnv(train_features.copy(), processed_data['train']['price'].copy())])\n",
    "                        train_env = VecNormalize(train_env, norm_obs=True, norm_reward=True)\n",
    "                        \n",
    "                        policy_kwargs = dict(\n",
    "                            features_extractor_class=AdvancedFeatureExtractor,\n",
    "                            features_extractor_kwargs=dict(features_dim=64, n_heads=4, dropout=0.1)\n",
    "                        )\n",
    "                        \n",
    "                        final_params = {k: v for k, v in best_params.items() if k not in ['train_env', 'score']}\n",
    "                        model = algo_class('MlpPolicy', train_env, verbose=0, policy_kwargs=policy_kwargs, **final_params)\n",
    "                        model.learn(total_timesteps=TRAINING_TIMESTEPS['final_training'])\n",
    "                        \n",
    "                        # Test evaluation\n",
    "                        print(f\"      üìä Testing...\")\n",
    "                        test_env = SimpleTradingEnv(test_features.copy(), processed_data['test']['price'].copy())\n",
    "                        \n",
    "                        obs = test_env.reset()\n",
    "                        done = False\n",
    "                        step_count = 0\n",
    "                        \n",
    "                        while not done and step_count < len(test_features):\n",
    "                            if hasattr(train_env, 'obs_rms'):\n",
    "                                normalized_obs = (obs - train_env.obs_rms.mean) / np.sqrt(train_env.obs_rms.var + 1e-8)\n",
    "                            else:\n",
    "                                normalized_obs = obs\n",
    "                            \n",
    "                            action, _ = model.predict(normalized_obs.reshape(1, -1), deterministic=True)\n",
    "                            obs, reward, done, info = test_env.step(action[0] if hasattr(action, '__getitem__') else action)\n",
    "                            step_count += 1\n",
    "                        \n",
    "                        # Results\n",
    "                        test_balance = test_env.get_balance_history()\n",
    "                        test_metrics = calculate_complete_metrics(test_balance, processed_data['test']['price'], benchmark_returns)\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'Method_Type': 'Reinforcement_Learning',\n",
    "                            'Wavelet_Family': family_name,\n",
    "                            'Wavelet': wavelet,\n",
    "                            'Algorithm': algo_name,\n",
    "                            'Features': feature_name,\n",
    "                            'Feature_Count': len(feature_indices),\n",
    "                            'Feature_Extractor': 'Advanced',\n",
    "                            'Strategy': 'Long-Only',\n",
    "                            'Training_Timesteps': TRAINING_TIMESTEPS['final_training'],\n",
    "                            'Random_Seed': algo_seed,  # A√ëADIDO\n",
    "                            **test_metrics,\n",
    "                            'Best_Learning_Rate': best_params.get('learning_rate', 0),\n",
    "                            'Best_Gamma': best_params.get('gamma', 0),\n",
    "                            'Validation_Score': best_params.get('score', 0)\n",
    "                        }\n",
    "                        \n",
    "                        all_rl_results.append(result)\n",
    "                        \n",
    "                        print(f\"      ‚úÖ Sharpe: {test_metrics['Sharpe_Ratio']:.3f}, Return: {test_metrics['Total_Return']*100:.1f}%\")\n",
    "                        \n",
    "                        # Cleanup\n",
    "                        train_env.close()\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      ‚ùå Error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            # Classical methods (once per wavelet, using best feature combination)\n",
    "            print(f\"\\n  üèõÔ∏è Classical Methods for {wavelet}\")\n",
    "            try:\n",
    "                best_features = processed_data['train']['indicators']  # Use all features\n",
    "                test_features_classical = processed_data['test']['indicators']\n",
    "                \n",
    "                # Create labels for supervised learning\n",
    "                train_labels = create_trading_labels(processed_data['train']['price'])\n",
    "                # Ensure features and labels have same length\n",
    "                min_len = min(len(best_features), len(train_labels))\n",
    "                best_features = best_features[:min_len]\n",
    "                train_labels = train_labels[:min_len]\n",
    "                \n",
    "                classical_results = implement_classical_methods(\n",
    "                    best_features, train_labels, test_features_classical, processed_data['test']['price']\n",
    "                )\n",
    "                \n",
    "                for method_name, metrics in classical_results.items():\n",
    "                    classical_result = {\n",
    "                        'Method_Type': 'Classical',\n",
    "                        'Wavelet_Family': family_name,\n",
    "                        'Wavelet': wavelet,\n",
    "                        'Algorithm': method_name,\n",
    "                        'Features': 'DIX_GEX_VIX',\n",
    "                        'Feature_Count': 3,\n",
    "                        'Feature_Extractor': 'None',\n",
    "                        'Strategy': 'Long-Only',\n",
    "                        'Training_Timesteps': 0,\n",
    "                        'Random_Seed': 0,  # A√ëADIDO\n",
    "                        **metrics,\n",
    "                        'Best_Learning_Rate': 0,\n",
    "                        'Best_Gamma': 0,\n",
    "                        'Validation_Score': 0\n",
    "                    }\n",
    "                    all_classical_results.append(classical_result)\n",
    "                \n",
    "                print(f\"      ‚úÖ Classical methods completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ùå Classical methods error: {str(e)}\")\n",
    "                continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {wavelet}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# ==============================================================\n",
    "# SAVE COMPLETE RESULTS\n",
    "# ==============================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üíæ SAVING COMPLETE RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_rl_results or all_classical_results:\n",
    "    # Combine all results\n",
    "    all_results = all_rl_results + all_classical_results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save main results\n",
    "    main_csv = os.path.join(results_dir, 'out_of_sample', 'complete_results_with_classical_methods_FIXED.csv')\n",
    "    results_df.to_csv(main_csv, index=False)\n",
    "    print(f\"‚úÖ Complete Results saved: {main_csv}\")\n",
    "    \n",
    "    # Save denoising analysis\n",
    "    if denoising_analysis_results:\n",
    "        denoising_df = pd.DataFrame(denoising_analysis_results)\n",
    "        denoising_csv = os.path.join(results_dir, 'denoising_analysis', 'denoising_effectiveness.csv')\n",
    "        denoising_df.to_csv(denoising_csv, index=False)\n",
    "        print(f\"‚úÖ Denoising analysis saved: {denoising_csv}\")\n",
    "    \n",
    "    # Statistical significance analysis\n",
    "    significance_path = os.path.join(results_dir, 'academic_reports', 'statistical_significance.txt')\n",
    "    statistical_significance_analysis(results_df, significance_path)\n",
    "    \n",
    "    # VERIFICAR DIFERENCIAS ENTRE ALGORITMOS\n",
    "    print(f\"\\nüîç VERIFICANDO RESULTADOS:\")\n",
    "    rl_results = results_df[results_df['Method_Type'] == 'Reinforcement_Learning']\n",
    "    \n",
    "    for algo in ['A2C', 'DQN', 'PPO']:\n",
    "        algo_data = rl_results[rl_results['Algorithm'] == algo]\n",
    "        if len(algo_data) > 0:\n",
    "            best_result = algo_data.loc[algo_data['Sharpe_Ratio'].idxmax()]\n",
    "            print(f\"  {algo}: Sharpe={best_result['Sharpe_Ratio']:.4f}, \"\n",
    "                  f\"Seed={best_result['Random_Seed']}, \"\n",
    "                  f\"Wavelet={best_result['Wavelet']}, \"\n",
    "                  f\"Features={best_result['Features']}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ CORRECCIONES APLICADAS:\")\n",
    "    print(f\"  ‚úì Semillas aleatorias √∫nicas por experimento\")\n",
    "    print(f\"  ‚úì Reset completo del environment\")\n",
    "    print(f\"  ‚úì M√°s variaci√≥n en hiperpar√°metros A2C/DQN\")\n",
    "    print(f\"  ‚úì Normalizaci√≥n mejorada\")\n",
    "    print(f\"  ‚úì Registro de semillas en resultados\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results to save. Check for errors in the processing loop.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ COMPLETE ANALYSIS FINISHED!\")\n",
    "print(f\"üìÅ All results saved in: {results_dir}\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
